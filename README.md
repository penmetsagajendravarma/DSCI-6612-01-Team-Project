# Pixelâ€‘Based Deepâ€¯Reinforcementâ€¯Learning for **Flappyâ€¯Bird** <img src="https://img.shields.io/badge/rl-QR--DQN-blue"> <img src="https://img.shields.io/badge/license-MIT-green">

> **One tiny Inception block â†’â€¯+135â€¯% average score**

Train a Flappyâ€¯Bird agent **directly from raw pixels** with [QRâ€‘DQN](https://arxiv.org/abs/1710.10044).
Two CNN backâ€‘bones, Colabâ€‘oneâ€‘click, and GPUâ€‘friendly codeâ€”because flapping shouldnâ€™t be a hassle.

---

## Tableâ€¯ofâ€¯Contents

1. [Features](#features)
2. [Demo](#demo)
3. [Quickâ€¯Start](#quick-start)
4. [Repositoryâ€¯Layout](#repository-layout)
5. [Hyperâ€‘parameters](#hyper-parameters)
6. [Results](#results)
7. [License](#license)
8. [Citingâ€¯&â€¯Thanks](#citing--thanks)

---

## Features

* **Pixelâ€‘only input** â€“ zero handâ€‘engineered signals.
* **Distributional QRâ€‘DQN** â€“ robust value estimates.
* **Swappable CNNs** â€“ Atari baseline vsâ€¯Inception upgrade.
* **Oneâ€‘command Colab** â€“ GPU training & MP4 demo in minutes.
* Clean, modular Python packageâ€”drop it into your own RL projects.

---

## Demo

<p align="center">
  <img src="docs/demo.gif" alt="Demo GIF" width="600">
</p>

---

## Quickâ€¯Start

### â–¶ï¸ Run in Colab

Hit the **â€œRunâ€¯inâ€¯Colabâ€** badge in the notebook. It installs deps, trains, and plays a demo videoâ€”done.

### ğŸ–¥ï¸ Run Locally

```bash
git clone https://github.com/<your-user>/flappy-pixel-drl.git
cd flappy-pixel-drl

# (Optional) isolate packages
python -m venv .venv && source .venv/bin/activate

pip install -r requirements.txt        # ships with PyTorchâ€‘cu11 wheels

# Train Inception model for 2â€¯M steps
python train.py --model inception --steps 2_000_000

# Evaluate the trained checkpoint
python evaluate.py checkpoints/inception_2M.zip
```

---

## Repositoryâ€¯Layout

```
.
â”œâ”€ flappy/                 # game + Gym wrapper
â”‚  â”œâ”€ core.py              # Flappy physics
â”‚  â”œâ”€ env_pixel.py         # FlappyPixelEnv (Gym API)
â”‚  â””â”€ extractor.py         # Atari & Inception CNNs
â”œâ”€ train.py                # CLI trainer
â”œâ”€ evaluate.py             # CLI evaluator
â”œâ”€ play.py                 # records MP4 demo
â”œâ”€ notebook.ipynb          # tutorial / slides
â”œâ”€ requirements.txt
â””â”€ checkpoints/
   â”œâ”€ atari_qrdqn.zip
   â””â”€ inception_qrdqn.zip
```

---

## Hyperâ€‘parameters

| Group                 | Value          | Why                                      |
| --------------------- | -------------- | ---------------------------------------- |
| Replay buffer / batch | 50â€¯k /â€¯32      | Fits a 16â€¯GB GPU, mirrors Atari configs  |
| Learning rate         | 5â€¯Ã—â€¯10â»â´       | Stable sweetâ€‘spot for QRâ€‘DQN             |
| Targetâ€‘net update     | 1â€¯kâ€¯steps      | Keeps learning steady, avoids divergence |
| Îµâ€‘schedule            | 1 â†’â€¯0.1 (15â€¯%) | Heavy exploration early when it matters  |
| Reward shaping        | +0.2â€¯/â€¯frame   | Survival bonus fights sparse rewards     |

Full table lives in `docs/hparams.md`.

---

## Results â€“ 2â€¯M steps on a single A100

| Backbone      | Avgâ€¯pipes â†‘ | Bestâ€¯pipes | Avgâ€¯frames â†‘ |     TTFP â†“ |
| ------------- | ----------: | ---------: | -----------: | ---------: |
| Atari CNN     |         5.6 |         22 |          299 |     1.30â€¯M |
| **Inception** |    **13.2** |     **61** |      **606** | **0.78â€¯M** |

*TTFP = frames until clearing the first pipe.*

TensorBoard logs are in `tb/`.

---

## License

MIT Â©â€¯2025â€¯\<Yourâ€¯Nameâ€¯/â€¯University>

---

## Citingâ€¯&â€¯Thanks

If this repo flaps its way into your research, please cite:

```bibtex
@misc{flappy_pixel_drl_2025,
  author       = {<Your Name>},
  title        = {Pixel-Based Deep Reinforcement Learning for Flappy Bird},
  year         = {2025},
  howpublished = {\url{https://github.com/<your-user>/flappy-pixel-drl}}
}
```

Inspired by the Atari Zoo and DeepMind Control Suite work.
Happy flapping! ğŸ¦
